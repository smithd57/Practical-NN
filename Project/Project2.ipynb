{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import load_cifar_template as lc\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import scipy.io as spio\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import timeit\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 333/333 [00:17<00:00, 19.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#fetch the data\n",
    "\n",
    "def fetch_data(directory, label): \n",
    "    item_list = []\n",
    "    step = 0\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        full_img_str = directory + \"/\" + file\n",
    "        #print(full_img_str)\n",
    "\n",
    "        mat = spio.loadmat(full_img_str, squeeze_me=True)\n",
    "        data = np.abs(mat[\"data_store\"])\n",
    "        smaller_data = data[200:300]\n",
    "        \n",
    "        image_resized = resize(smaller_data, (100, 320, 4),\n",
    "                       anti_aliasing=True)\n",
    "        \n",
    "        sub_list = []\n",
    "        for i in range(8):\n",
    "            chunk = image_resized[:,40*i:40*i+40]\n",
    "            sub_list.append(chunk)\n",
    "        \n",
    "        #\n",
    "        #print(data.shape)\n",
    "        #print(data)\n",
    "        ###append the img and label to the list###\n",
    "        #sub_list = [image_resized, label]\n",
    "        #print(sub_list)\n",
    "        item_list.append([sub_list, label])\n",
    "        \n",
    "    return item_list\n",
    "    \n",
    "bike1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1000/data\", [1, 0, 0])\n",
    "bike2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1001/data\", [1, 0, 0])\n",
    "bike3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1002/data\", [1, 0, 0])\n",
    "\n",
    "car1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1000/data\", [0, 1, 0])\n",
    "car2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1001/data\", [0, 1, 0])\n",
    "car3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1002/data\", [0, 1, 0])\n",
    "\n",
    "ped1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms1000/data\", [0, 0, 1])\n",
    "ped2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms1001/data\", [0, 0, 1])\n",
    "ped3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms2000/data\", [0, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "#directory = \"Desktop/ee596prepro/2019_04_09_bms1000/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2997, 2)\n"
     ]
    }
   ],
   "source": [
    "def mini_batch(features,labels,mini_batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        features: features for one batch\n",
    "        labels: labels for one batch\n",
    "        mini_batch_size: the mini-batch size you want to use.\n",
    "    Hint: Use \"yield\" to generate mini-batch features and labels\n",
    "    \"\"\"\n",
    "    #split the data into batches\n",
    "    amount_of_data = len(features)\n",
    "    number_of_bunches = amount_of_data/mini_batch_size\n",
    "    \n",
    "    bunches_features = []\n",
    "    bunches_labels = []\n",
    "    \n",
    "    #loop over breaking the data into batches\n",
    "    for i in range(int(number_of_bunches)):\n",
    "        current_range = i * mini_batch_size\n",
    "        f_b = features[current_range:current_range+mini_batch_size]\n",
    "        l_b = labels[current_range:current_range+mini_batch_size]\n",
    "        \n",
    "        bunches_features.append(f_b)\n",
    "        bunches_labels.append(l_b)\n",
    "    \n",
    "    #return the mini-batched data\n",
    "    return bunches_features, bunches_labels\n",
    "\n",
    "#full_list = bike1 + bike2 + bike3 + car1 + car2 + car3 + ped1 + ped2 + ped3\n",
    "full_list = bike1 + car1 + ped1 + bike2 + car2 + ped2 + bike3 + car3 + ped3\n",
    "#test_list = bike3 + car3 + ped3\n",
    "\n",
    "\n",
    "#test_list = bike3 + car3 + ped3\n",
    "\n",
    "np.random.shuffle(full_list)\n",
    "#np.random.shuffle(test_list)\n",
    "print(np.asarray(full_list).shape)\n",
    "#print(np.asarray(full_list)[0][0].shape)\n",
    "#np.random.shuffle(test_list)\n",
    "\n",
    "training_set = full_list[0:2100]\n",
    "valid_set = full_list[2100:2550]\n",
    "test_set = full_list[2550:2997]\n",
    "#full_list = None\n",
    "#bike1 = None\n",
    "#car1 = None\n",
    "#ped1 = None\n",
    "\n",
    "train_set_data = np.zeros((len(training_set), 8, 100, 40, 4))\n",
    "train_set_labels = np.zeros((len(training_set), 3))\n",
    "valid_set_data = np.zeros((len(valid_set), 8, 100, 40, 4))\n",
    "valid_set_labels = np.zeros((len(valid_set), 3))\n",
    "test_set_data = np.zeros((len(test_set), 8, 100, 40, 4))\n",
    "test_set_labels = np.zeros((len(test_set), 3))\n",
    "#print(np.asarray(training_set).shape)\n",
    "#print(training_set[0][0][0])\n",
    "#print(training_set[0][:][1])\n",
    "#print(training_set[0][:][0].shape)\n",
    "#print(training_set[0][:][0])\n",
    "#split into training, valid, and testing\n",
    "for i in range(len(training_set)):\n",
    "    train_set_data[i] = training_set[i][0]#np.reshape(training_set[i][0], (100, 960, 4))\n",
    "    train_set_labels[i] = training_set[i][1]\n",
    "    \n",
    "for i in range(len(valid_set)):\n",
    "    valid_set_data[i] = valid_set[i][0]#np.reshape(valid_set[i][0], (100, 960, 4))\n",
    "    valid_set_labels[i] = valid_set[i][1]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_set_data[i] = test_set[i][0]#np.reshape(test_set[i][0], (100, 960, 4))\n",
    "    test_set_labels[i] = test_set[i][1]\n",
    "\n",
    "#print(np.asarray(train_set_data).shape)\n",
    "#print(train_set_data.shape)\n",
    "\n",
    "\n",
    "#train_set_data, train_set_labels = mini_batch(train_set_data,train_set_labels,5)\n",
    "#valid_set_data, valid_set_labels = mini_batch(valid_set_data,valid_set_labels,5)\n",
    "#test_set_data, test_set_labels = mini_batch(test_set_data,test_set_labels,5)\n",
    "\n",
    "#training_set = None\n",
    "#valid_set = None\n",
    "#test_set = None\n",
    "\n",
    "\n",
    "#test_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 40, 4)\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_set_data[0].shape)\n",
    "print(train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 8, 100, 40, 4)\n",
      "Train on 2100 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      " - 24s - loss: 0.8492 - mean_absolute_error: 0.3576 - categorical_accuracy: 0.6071 - val_loss: 0.7144 - val_mean_absolute_error: 0.3030 - val_categorical_accuracy: 0.7156\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.5751 - mean_absolute_error: 0.2501 - categorical_accuracy: 0.7933 - val_loss: 0.5167 - val_mean_absolute_error: 0.2228 - val_categorical_accuracy: 0.7800\n",
      "Epoch 3/10\n",
      " - 22s - loss: 0.3998 - mean_absolute_error: 0.1821 - categorical_accuracy: 0.8633 - val_loss: 0.3728 - val_mean_absolute_error: 0.1640 - val_categorical_accuracy: 0.8689\n",
      "Epoch 4/10\n",
      " - 22s - loss: 0.2870 - mean_absolute_error: 0.1343 - categorical_accuracy: 0.9086 - val_loss: 0.2774 - val_mean_absolute_error: 0.1274 - val_categorical_accuracy: 0.9156\n",
      "Epoch 5/10\n",
      " - 22s - loss: 0.2198 - mean_absolute_error: 0.1055 - categorical_accuracy: 0.9371 - val_loss: 0.2205 - val_mean_absolute_error: 0.1054 - val_categorical_accuracy: 0.9333\n",
      "Epoch 6/10\n",
      " - 22s - loss: 0.1761 - mean_absolute_error: 0.0875 - categorical_accuracy: 0.9538 - val_loss: 0.1725 - val_mean_absolute_error: 0.0852 - val_categorical_accuracy: 0.9533\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.1438 - mean_absolute_error: 0.0734 - categorical_accuracy: 0.9657 - val_loss: 0.1459 - val_mean_absolute_error: 0.0742 - val_categorical_accuracy: 0.9600\n",
      "Epoch 8/10\n",
      " - 22s - loss: 0.1213 - mean_absolute_error: 0.0632 - categorical_accuracy: 0.9733 - val_loss: 0.1169 - val_mean_absolute_error: 0.0621 - val_categorical_accuracy: 0.9800\n",
      "Epoch 9/10\n",
      " - 22s - loss: 0.1003 - mean_absolute_error: 0.0540 - categorical_accuracy: 0.9805 - val_loss: 0.1008 - val_mean_absolute_error: 0.0531 - val_categorical_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      " - 22s - loss: 0.0887 - mean_absolute_error: 0.0482 - categorical_accuracy: 0.9852 - val_loss: 0.1054 - val_mean_absolute_error: 0.0521 - val_categorical_accuracy: 0.9711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a9e82a3048>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#import keras.models\n",
    "#import keras.layers\n",
    "from tensorflow.keras import metrics\n",
    "#X = seq.reshape(len(seq), 1, 1)\n",
    "#y = seq.reshape(len(seq), 1)\n",
    "seq_imgs = 25\n",
    "print(np.asarray(train_set_data).shape)\n",
    "#train_set_data = np.reshape(train_set_data, (980*25, 227, 227, 3))\n",
    "#print(train_set_labels.shape)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(30, (5, 5), activation=\"relu\"),input_shape=(8, 100, 40, 4)))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(2, (2, 2))))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
    "model.add(tf.keras.layers.LSTM(50))\n",
    "model.add(tf.keras.layers.Dense(3, activation = 'softmax'))\n",
    "\n",
    "adam_opt = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#7\n",
    "#sgd_opt = tf.keras.optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_opt,\n",
    "              metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "\n",
    "model.fit(train_set_data, train_set_labels, epochs=10, batch_size=5, verbose = 2, validation_data = (valid_set_data, valid_set_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.966\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "\n",
    "test_set_data = np.asarray(test_set_data)\n",
    "test_set_labels = np.asarray(test_set_labels)\n",
    "\n",
    "#fetch batch\n",
    "#batch_x = test_set_data[k]\n",
    "#batch_y = test_set_labels[k]\n",
    "#run optimization\n",
    "prediction = model.predict(test_set_data, verbose=0)\n",
    "#print(prediction)\n",
    "for k in range(len(test_set_data)):\n",
    "    if (np.argmax(prediction[k]) == np.argmax(test_set_labels[k])):\n",
    "        acc += 1\n",
    "    else:\n",
    "        acc += 0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "acc = acc/len(test_set_data)\n",
    "print(\"Test Accuracy= {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
