{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import load_cifar_template as lc\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the data\n",
    "\n",
    "def fetch_data(directory, label): \n",
    "    item_list = []\n",
    "    step = 0\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        full_img_str = directory + \"/\" + file\n",
    "        #print(full_img_str)\n",
    "\n",
    "        mat = spio.loadmat(full_img_str, squeeze_me=True)\n",
    "        data = np.abs(mat[\"data_store\"])\n",
    "        smaller_data = data[100:400]\n",
    "        \n",
    "        image_resized = resize(smaller_data, (100, 320, 4),\n",
    "                       anti_aliasing=True)\n",
    "        #print(data.shape)\n",
    "        #print(data)\n",
    "        ###append the img and label to the list###\n",
    "        sub_list = [image_resized, label]\n",
    "        #print(sub_list)\n",
    "        item_list.append(sub_list)\n",
    "        \n",
    "    return item_list\n",
    "    \n",
    "bike1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1000/data\", [1, 0, 0])\n",
    "bike2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1001/data\", [1, 0, 0])\n",
    "bike3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_bms1002/data\", [1, 0, 0])\n",
    "\n",
    "car1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1000/data\", [0, 1, 0])\n",
    "car2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1001/data\", [0, 1, 0])\n",
    "car3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_cms1002/data\", [0, 1, 0])\n",
    "\n",
    "ped1 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms1000/data\", [0, 0, 1])\n",
    "ped2 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms1001/data\", [0, 0, 1])\n",
    "ped3 = fetch_data(\"Desktop/ee596prepro/2019_04_09_pms2000/data\", [0, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "#directory = \"Desktop/ee596prepro/2019_04_09_bms1000/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(features,labels,mini_batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        features: features for one batch\n",
    "        labels: labels for one batch\n",
    "        mini_batch_size: the mini-batch size you want to use.\n",
    "    Hint: Use \"yield\" to generate mini-batch features and labels\n",
    "    \"\"\"\n",
    "    #split the data into batches\n",
    "    amount_of_data = len(features)\n",
    "    number_of_bunches = amount_of_data/mini_batch_size\n",
    "    \n",
    "    bunches_features = []\n",
    "    bunches_labels = []\n",
    "    \n",
    "    #loop over breaking the data into batches\n",
    "    for i in range(int(number_of_bunches)):\n",
    "        current_range = i * mini_batch_size\n",
    "        f_b = features[current_range:current_range+mini_batch_size]\n",
    "        l_b = labels[current_range:current_range+mini_batch_size]\n",
    "        \n",
    "        bunches_features.append(f_b)\n",
    "        bunches_labels.append(l_b)\n",
    "    \n",
    "    #return the mini-batched data\n",
    "    return bunches_features, bunches_labels\n",
    "\n",
    "#full_list = bike1 + bike2 + bike3 + car1 + car2 + car3 + ped1 + ped2 + ped3\n",
    "full_list = bike1 + car1 + ped1 + bike2 + car2 + ped2# + bike3 + car3 + ped3\n",
    "test_list = bike3 + car3 + ped3\n",
    "\n",
    "np.random.shuffle(full_list)\n",
    "np.random.shuffle(test_list)\n",
    "\n",
    "training_set = full_list#[0:2000]\n",
    "valid_set = test_list[0:500]\n",
    "test_set = test_list[500:997]\n",
    "\n",
    "\n",
    "train_set_data = []\n",
    "train_set_labels = []\n",
    "valid_set_data = []\n",
    "valid_set_labels = []\n",
    "test_set_data = []\n",
    "test_set_labels = []\n",
    "#print(np.asarray(training_set).shape)\n",
    "#print(training_set[0][0][0])\n",
    "#print(training_set[0][:][1])\n",
    "#print(training_set[0][:][0].shape)\n",
    "#print(training_set[0][:][0])\n",
    "#split into training, valid, and testing\n",
    "for i in range(len(training_set)):\n",
    "    train_set_data.append(training_set[i][0])\n",
    "    train_set_labels.append(training_set[i][1])\n",
    "    \n",
    "for i in range(len(valid_set)):\n",
    "    valid_set_data.append(valid_set[i][0])\n",
    "    valid_set_labels.append(valid_set[i][1])\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_set_data.append(test_set[i][0])\n",
    "    test_set_labels.append(test_set[i][1])\n",
    "\n",
    "#print(np.asarray(train_set_data).shape)\n",
    "#print(train_set_data.shape)\n",
    "\n",
    "\n",
    "train_set_data, train_set_labels = mini_batch(train_set_data,train_set_labels,5)\n",
    "valid_set_data, valid_set_labels = mini_batch(valid_set_data,valid_set_labels,5)\n",
    "test_set_data, test_set_labels = mini_batch(test_set_data,test_set_labels,5)\n",
    "\n",
    "#training_set = None\n",
    "#valid_set = None\n",
    "#test_set = None\n",
    "#full_list = None\n",
    "#bike1 = None\n",
    "#car1 = None\n",
    "#ped1 = None\n",
    "#test_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 25, 227, 227, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected time_distributed_3_input to have 5 dimensions, but got array with shape (24500, 227, 227, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-651ae8d0545c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\douglas_smith\\anaconda3\\envs\\ee596\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\douglas_smith\\anaconda3\\envs\\ee596\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\douglas_smith\\anaconda3\\envs\\ee596\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    351\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected time_distributed_3_input to have 5 dimensions, but got array with shape (24500, 227, 227, 3)"
     ]
    }
   ],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import LSTM\n",
    "#import keras.models\n",
    "#import keras.layers\n",
    "\n",
    "#X = seq.reshape(len(seq), 1, 1)\n",
    "#y = seq.reshape(len(seq), 1)\n",
    "seq_imgs = 25\n",
    "print(np.asarray(train_set_data).shape)\n",
    "#train_set_data = np.reshape(train_set_data, (980*25, 227, 227, 3))\n",
    "#print(train_set_labels.shape)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(2, (5, 5)),\n",
    "                              input_shape=(seq_imgs, 227, 227, 3)))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(2, (2, 2))))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
    "model.add(tf.keras.layers.LSTM(50))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(train_set_data, train_set_labels, epochs=20, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
