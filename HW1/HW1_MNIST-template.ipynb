{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 1000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 18000\n",
    "n_hidden_2 = 6000\n",
    "#n_hidden_3 = 1000\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    #'W3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3]),name='W3'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    #'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define neural network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    #layer_3_out = tf.nn.relu(tf.add(tf.matmul(layer_2_out,weights['W3']),biases['b3']))\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define cost function and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Accuracy= 0.289\n",
      "step 40, Accuracy= 0.539\n",
      "step 80, Accuracy= 0.969\n",
      "step 120, Accuracy= 0.984\n",
      "step 160, Accuracy= 0.977\n",
      "step 200, Accuracy= 1.000\n",
      "step 240, Accuracy= 0.953\n",
      "step 280, Accuracy= 0.969\n",
      "step 320, Accuracy= 0.992\n",
      "step 360, Accuracy= 0.992\n",
      "step 400, Accuracy= 0.992\n",
      "step 440, Accuracy= 0.992\n",
      "step 480, Accuracy= 1.000\n",
      "step 520, Accuracy= 0.984\n",
      "step 560, Accuracy= 0.969\n",
      "step 600, Accuracy= 1.000\n",
      "step 640, Accuracy= 0.977\n",
      "step 680, Accuracy= 0.984\n",
      "step 720, Accuracy= 1.000\n",
      "step 760, Accuracy= 0.992\n",
      "step 800, Accuracy= 1.000\n",
      "step 840, Accuracy= 0.992\n",
      "step 880, Accuracy= 1.000\n",
      "step 920, Accuracy= 1.000\n",
      "step 960, Accuracy= 1.000\n",
      "Training finished!\n",
      "Testing Accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if i % (num_steps / 25) ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy at different epochs and the accuracy of the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Your results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Lr\t    |Epoch|\tActivation|\tBatch size|\tHL 1 nodes|\tHL 2 nodes|\tAccuracy|\n",
    "|------|------|------|------|------|------|------|\n",
    "|0.01\t|500|\t    None|\t    128|\t        300|\t        100|\t        0.84|\n",
    "|0.01\t|50000|\tSigmoid\t|    128|\t        300|\t        100\t  |      0.88|\n",
    "|0.001\t|50000|\tSigmoid\t|    128|\t        300\t |       100\t |       0.71|\n",
    "|0.01\t|50000|\tReLu\t|    128|\t        300\t |       100\t |       0.89|\n",
    "|0.001\t|500000|\tReLU|\t    128|\t300|\t100|\t0.904|\n",
    "|0.01\t|5000|\tReLU\t|    128|\t3000|\t1000|\t0.929|\n",
    "|0.01\t|10000|\tReLU\t|    128|\t3000|\t1000|\t0.934|\n",
    "|0.001\t|50000|\tReLU\t|    128|\t3000|\t1000|\t0.948|\n",
    "|0.01\t|10000|\tReLU\t|    128|\t6000|\t2000|\t0.9511|\n",
    "|0.01\t|1000|\tReLU\t|    128|\t9000|\t3000|\t0.9473|\n",
    "|0.01\t|1000|\tReLU\t|    128|\t12000|\t4000|\t0.9537|\n",
    "|0.01\t|1000|\tReLU\t|    128|\t15000|\t5000|\t0.9608|\n",
    "|0.01\t|1000|\tReLU\t|    128|\t18000|\t6000|\t0.9623|\n",
    "\n",
    "\n",
    "In trying to get the accuracy of the testing data above 96% several attributes were modified and tested. These included the learning rate, the number of epochs, the activation functions, and the number of nodes in the hidden layers. The modifications that made the biggest difference were adding more nodes to the hidden layers and adding activation functions to the hidden layers. These changes improving the behavior of the system makes reasonable sense. Originally the system didn’t have activation functions between the layers, so the system was just summing everything together. Adding the activation functions completed the neural network's structure and let us get the full benefit of the two hidden layers. Adding more neurons improved the behavior because as more neurons were added the system gained more “thinking” power and the network could develop a better and more generalized classifier, allowing it to do better on the testing data. Decreasing the learning rate, increasing the number of epochs, and using ReLU over sigmoid only had minor to no benefit for performance. The minimal benefit of increasing the training time makes sense as the primary issue was that the system had high variance meaning the training accuracy was much better than the testing accuracy. In this scenario it is much more beneficial to make the network bigger than to increase the training time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
